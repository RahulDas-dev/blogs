<section class="container ">
  <h1 class="section__title">The math behind OLS [ Ordinary Least Squares ] regression</h1>

  <h1 class="section__subtitle">The general problem statement for Regression</h1>
  {{> regression_defination}}
  
  <h1 class="section__subtitle">The OLS regression</h1>
  {{> ols_defination}}

  <h1 class="section__subtitle">The General Loss Function of Regression Problem</h1>
    {{> regression_loss}}

  <h1 class="section__subtitle">The Loss Function of OLS Regression</h1>  
    <div>
    Substituting $\quad \hat{Y} = X\beta \quad$ from equation [2] in above equation $\\$
    $$ \mathcal{L} = \frac{1}{n}(Y^TY - Y^TX\beta - (X\beta)^TY + (X\beta)^T(X\beta)) $$

    Note that $(X\beta)^T = \beta^TX^T$: $\\$
    $$ \mathcal{L} = \frac{1}{n}(Y^TY - Y^TX\beta - \beta^TX^TY + \beta^TX^TX\beta) $$

    Since $Y^TX\beta$ is a scalar, $Y^TX\beta = (Y^TX\beta)^T = \beta^TX^TY$: $\\$
    $$ \mathcal{L} = \frac{1}{n}(Y^TY - 2Y^TX\beta + \beta^TX^TX\beta)$$

    The Loss function $\mathcal{L}$ can be expressed as a function of $Y$, $X$ and $\beta$: $\\$

    $$ \mathcal{L}(Y, X, \beta) = \frac{1}{n}(Y^TY - 2Y^TX\beta + \beta^TX^TX\beta) \tag{3}$$

    where: $\\$
    $Y \in \mathbb{R}^{n \times 1}$ is the vector of observed values $\\$
    $X \in \mathbb{R}^{n \times (m+1)}$ is the design matrix $\\$
    $\beta \in \mathbb{R}^{(m+1) \times 1}$ is the parameter vector $\\$
    </div>

  <h1 class="section__subtitle">Minimization of Loss Function</h1>  
    <div>
    To find the minimum of $\mathcal{L}(Y, X, \beta)$, we will take differentiation of $\mathcal{L}$, since $Y, X$ are constants for given dataset, we will take differentiation with respect to $\beta$. 
    Essentially we are asking which value of $\beta$ will make $\mathcal{L}$ minimum , i.e
    $$\frac{\partial \mathcal{L} }{\partial \beta} = 0$$ 

    $$ \frac{\partial \mathcal{L} }{\partial \beta} = \frac{1}{n}(-2X^TY + 2X^TX\beta) = 0 \tag{4}$$

    Solving for $\beta$: $\\$

    $$ -2X^TY + 2X^TX\beta = 0 $$
    $$ 2X^TX\beta = 2X^TY $$
    $$ X^TX\beta = X^TY $$
    $$ \beta = (X^TX)^{-1}X^TY \tag{5}$$

    <div class="item__title">Wait, </div>
    Since $\mathcal{L}: \mathbb{R}^{(m+1)} \to \mathbb{R}$ maps a vector to a scalar, we can use not simply use partial
    differentiation, we will must use the gradient operator: $\\$

    Starting from equation (2): $\\$
    
    $$ \mathcal{L}(Y, X, \beta) = \frac{1}{n}(Y^TY - 2Y^TX\beta + \beta^TX^TX\beta) $$

    Taking the gradient with respect to $\beta$: $\\$

    $$ \nabla_\beta \mathcal{\mathcal{L} } = \frac{1}{n}\nabla_\beta(Y^TY - 2Y^TX\beta + \beta^TX^TX\beta) $$

    Using matrix calculus rules: $\\$
    $$ \nabla_\beta(Y^TY) = 0 \quad \text{(constant term)} $$
    $$ \nabla_\beta(-2Y^TX\beta) = -2X^TY $$
    $$ \nabla_\beta(\beta^TX^TX\beta) = 2X^TX\beta $$

    Therefore: $\\$
    $$ \nabla_\beta \mathcal{L} = \frac{1}{n}(0 - 2X^TY + 2X^TX\beta) = \frac{1}{n}(-2X^TY + 2X^TX\beta) \tag{6} $$

    Solving for $\beta$: $\\$

    $$ -2X^TY + 2X^TX\beta = 0 $$
    $$ 2X^TX\beta = 2X^TY $$
    $$ X^TX\beta = X^TY $$
    $$ \beta = (X^TX)^{-1}X^TY \tag{7}$$
    </div>
    <h1 class="section__subtitle">How to verify the Minimum Value of Loss Function</h1>  
    <div>
    To verify this is a minimum, we need to show that the Hessian matrix is positive definite: $\\$

    The Hessian matrix is the gradient of the gradient: $\\$
    $$ H = \nabla^2_\beta \mathcal{L} = \nabla_\beta(\nabla_\beta L) $$

    From equation (3): $\\$
    $$ \nabla_\beta \mathcal{L} = \frac{1}{n}(-2X^TY + 2X^TX\beta) $$

    Taking gradient again: $\\$
    $$ H = \nabla^2_\beta \mathcal{L} = \frac{2}{n}X^TX $$

    This is a minimum if $H$ is positive definite. $H$ is positive definite if: $\\$
    1. $X^TX$ is symmetric (which it is by construction) $\\$
    2. For any non-zero vector $v$, $v^T(X^TX)v > 0$ $\\$

    When $X$ has full column rank: $\\$
    $$ v^T(X^TX)v = (Xv)^T(Xv) = \|Xv\|^2 > 0 \quad \text{for all } v \neq 0 $$

    Therefore, $\hat{\beta} = (X^TX)^{-1}X^TY$ is indeed a global minimum when $X$ has full column rank. $\\$

  </div>

</section>