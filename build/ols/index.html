<!DOCTYPE html>
<html lang="en" class="dark">
<head>
    <title>The Math Behind OLS Regression</title>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
    <meta name="Description" CONTENT="">
    <meta name="robots" content="index, follow"/>
    <meta
      name="viewport"
      content="width=device-width, height=device-height,
                                  initial-scale=1.0, user-scalable=no,
                                  minimum-scale=1.0, maximum-scale=1.0"
    />
    <meta name="title" content="The Math Behind OLS [ Ordinary Least Squares ] Regression"/>
    <meta name="keywords" content="OLs, OLS regression, Ordinary Least Squares, Loss function, Gradient, Parametric model">
    <link rel="shotcut icon" type="image/png" href="../favicon.ico" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="The Math Behind OLS [ Ordinary Least Squares ] Regression"/>
    <meta property="og:image" content="../favicon-192x192.png" />
    <meta property="og:image:type" content="image/png" />
    <meta property="og:image:width" content="192" />
    <meta property="og:image:height" content="192" />
    <meta name="theme-color" content="#f8f9f9"/>
  <script type="module" crossorigin src="/blogs/assets/ols-CSYQrsBN.js"></script>
  <link rel="modulepreload" crossorigin href="/blogs/assets/auto-render-DFiRW_5p.js">
  <link rel="stylesheet" crossorigin href="/blogs/assets/auto-render-Bpznw4TJ.css">
  <link rel="stylesheet" crossorigin href="/blogs/assets/ols-DaZQC8CB.css">
</head>
<body>
<!--==================== Header ====================-->  
  <header id='header-top'>    
    <nav >
        <a href="https://rahuldas-dev.github.io/profile" target="_blank">
            <span class="nav__logo">Rahul Das
                <!--  <img src="assets/logo.png" alt="logo image" > -->
            </span>
        </a>
        <ul class="nav__menu hide-menu" id ="nav-menu">
            <li class="nav__item">
                <button class="nav__link" id="theme_btn">
                    <i class="icon-sun svg-icon" id="theme_icon"></i>
                </button>
            </li>
        </ul>
    </nav>
</header>  <main class="main" id="blog_contain">
    <section class="container ">
  <h1 class="section__title">The Math Behind OLS [ Ordinary Least Squares ] Regression</h1>

  <h1 class="section__subtitle">The General Problem Statement For Regression</h1>
  <div>
    <div class="item__title">Given:</div>
    1. A dataset $ {D= \{(x_1, y_1), (x_2, y_2), \cdots,(x_n, y_n)\}}$ of size $n$ where : $\\$
        $\quad \centerdot$ ${x_i = (x_i^1, x_i^2, \cdots, x_i^m)}$ represents the feature vector for the $i^{th}$ sample.  ${x_i \in \mathbb{R}^m}$$\\$  
        $\quad \centerdot$ and ${y_i}$ represents the corresponding target (output) value ${y_i \in \mathbb{R}}$ is a target.$\\$
    2. And a Loss function $L(y_i, \hat{y}_i)$ where $L : \mathbb{R} \times \mathbb{R} \to \mathbb{R}$. $\\$

    <br/>
    <div class="item__title">Objective:</div>
    The goal of regression is to find a function $F : \mathbb{R}^m \to \mathbb{R}$ that maps the input features $x_i$ to the target values $y_i$ 
    such that predicted value $\hat{y}_i = F(x_i)$ is as close to the target value $y_i$ as possible. i.e minimizes the Loss function $L(y_i, \hat{y}_i)$. $\\$

    <br/>
    <div class="item__title">Mathematical Formulation:</div>
    The problem can be expressed as minimizing a loss function or expected loss ${\mathcal{L}(D,F) := \mathbb{E}L(y, F(x))}$. The $\mathcal{L}$ quantifies the 
    difference between the true values $y_i$ and the predicted values $\hat{y}_i$ for each example in the dataset $D$. 

    $$ \text{Minimize:} \quad {\mathcal{L}(D,F) = \frac{1}{n}\mathbb{E}L(y, F(x)) = \frac{1}{n}\sum_{i=1}^n L(y_i, F(x_i))}$$
  
    <br/>
    <div class="item__title">Assumptions:</div>
    $\quad \centerdot \quad \{x_i, y_i\} \quad $ are independent and identically distributed according to some unknown distribution ${P(·, ·)}$ $\\$
    $\quad \centerdot$  The loss function $L$ is differentiable (in most cases) to enable optimization. $\\$ 
    <br/>

    <div class="item__title">Notations:</div>
    $\quad \centerdot \quad {D= \{(x_i, y_i)\}_{i=1..n} \qquad x_i \in \mathbb{R}^m \qquad y_i \in \mathbb{R}} \\$
    $\quad \centerdot \quad n \rightarrow $ Training dataset size $\\$
    $\quad \centerdot \quad m \rightarrow $ Number of features $\\$
    $\quad \centerdot \quad F \rightarrow $ The learning task is to find a function $F$ that minimizes the expected loss $\mathcal{L}(F)$ $\\$
    $\quad \centerdot \quad F(x) \rightarrow \hat{y} \rightarrow$ estimation for the sample $x$ $\\$
    $\quad \centerdot \quad L \rightarrow L(y, F(x)) \rightarrow L(y, \hat{y})$ The Loss Function $\\$
</div>  
  <h1 class="section__subtitle">The OLS regression</h1>
  <div>
    The Liner Regression is a form of Parametric methods. Parametric methods makes assumptions about the functional form, or shape, of $F$. 
    For example, Liner Regression makes one very simple assumption is that, $F$ is linear in x. 
    $$\quad F(x_i) = \beta_0 + \beta_1x_i^1 + \beta_2x_i^2 + ... + \beta_mx_i^m .\quad \text{The betas are Learnable Parameter} $$
    <br/>

    Now lets redefine $F$ interms of Matrix notations

    $$F(x_i) = \beta_0 + \beta_1x_i^1 + \beta_2x_i^2 + ... + \beta_mx_i^m = \begin{bmatrix} 1 & x_i^1 & x_i^2 & \cdots & x_i^m \end{bmatrix}
    \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end{bmatrix} = \mathcal{X}_i\beta = \hat{y}_i \tag{1}$$

    Lets define the nomanclueture for $\mathcal{X}_i$ $\beta$ and $\hat{y}_i$
    <br/>

    $\quad 1.$ $\mathcal{X}_i$ is derived feature vector from the input $x_i$, $x_i$ has $m$ dimension, so $\mathcal{X}_i$ has $m+1$ dimension.  
    $$\mathcal{X}_i = \begin{bmatrix} 1 & x_i^1 & x_i^2 & \cdots & x_i^m \end{bmatrix}_{1 \times (m+1)}
    = \begin{bmatrix} 1 & x_i \end{bmatrix}_{1 \times (m+1)} \quad \mathcal{X}_i \in \mathbb{R}^{1 \times (m+1)}$$ 
    
    $\quad 2.$ The $\beta$ is the Leranable model Parameter has $m+1$ dimension
    $$\beta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end{bmatrix}_{(m+1) \times 1} 
    = \begin{bmatrix} 1 & \beta_0 & \beta_1 & \beta_2 & \cdots & \beta_m \end{bmatrix}_{1 \times (m+1)}^{T}
    \quad\beta \in \mathbb{R}^{(m+1) \times 1}$$

    $\quad 3.\quad \hat{y}_i$  The predicted value of $y_i$, $\hat{y}_i$ is scalar so $\hat{y}_i \in \mathbb{R} \\$
    <!-- Previous content remains -->

    <br/>
    We can rewrite the euqtion one For the entire dataset of n observations: $\\$

    $$\hat{y}_1 = \beta_0 + \beta_1x_1^1 + \beta_2x_1^2 + ... + \beta_mx_1^m $$
    $$\hat{y}_2 = \beta_0 + \beta_1x_2^1 + \beta_2x_2^2 + ... + \beta_mx_2^m $$
    $$\hat{y}_3 = \beta_0 + \beta_1x_3^1 + \beta_2x_3^2 + ... + \beta_mx_3^m $$
    $$\vdots$$
    $$\hat{y}_n = \beta_0 + \beta_1x_n^1 + \beta_2x_n^2 + ... + \beta_mx_n^m $$

    <br/>
    We can rewrite above n linear equations in terms of Matrix noations  
    $$
    \begin{bmatrix} \hat{y}_1 \\ \hat{y}_2 \\ \vdots \\ \hat{y}_n \end{bmatrix} =
    \begin{bmatrix}
    1 & x_1^1 & x_1^2 & \cdots & x_1^m \\
    1 & x_2^1 & x_2^2 & \cdots & x_2^m \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_n^1 & x_n^2 & \cdots & x_n^m
    \end{bmatrix}
    \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end{bmatrix}
    $$

    Time for some more nomanclueture$\\$
    <br/>

    $\quad 1.$ The row vector $\hat{Y}$ is called prediction vector  the $\hat{Y}$ has $n$ dimension.  
    $$\hat{Y} = \begin{bmatrix} \hat{y}_1 \\ \hat{y}_2 \\ \hat{y}_3 \\ \vdots \\ \hat{y}_n \end{bmatrix}_{n \times 1}  \quad\hat{Y} \in \mathbb{R}^{n \times 1}$$

    $\quad 2.$ The matrix $X$ is called design Matrix  the $X$ has $n \times (m+1)$ dimension. 
    $$X = \begin{bmatrix}
    1 & x_1^1 & x_1^2 & \cdots & x_1^m \\
    1 & x_2^1 & x_2^2 & \cdots & x_2^m \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_n^1 & x_n^2 & \cdots & x_n^m
    \end{bmatrix}_{n \times m+1} X \in \mathbb{R}^{n \times (m+1)} $$

    $\quad 3.$ The $\beta$ is the Leranable model Parameter has $m+1$ dimension 
    $$\beta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end{bmatrix}_{(m+1) \times 1} \quad \beta \in \mathbb{R}^{(m+1) \times 1}$$
    <!-- Previous content remains -->
    <br/>
    Finally The Closed form Equation Linear Regression is , given as  $\quad \hat{Y} = X\beta$

    $$ \hat{Y} = X\beta \tag{2}$$
</div>

  <h1 class="section__subtitle">The General Loss Function of Regression Problem</h1>
    <div>
    The loss function is the squared error i.e $\quad (y_i - \hat{y}_i)^2$ , which quantifies the 
    difference between the true values $y_i$ and the predicted values $\hat{y}_i$ for $k^{th}$ example in the dataset $D$.
    
    $$ L(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2$$
    if we take the expected value of loss over the entire training dataset loss function or expected loss becomes Cost function ${\mathcal{L}(D,F) := \mathbb{E}L(y, F(x))}$. The $\mathcal{L}$ quantifies the 
    difference between the true values $y_i$ and the predicted values $\hat{y}_i$ for each example in the dataset $D$.  

    $$ {\mathcal{L}(D,F) = \frac{1}{n}\mathbb{E}L(y, F(x)) = \frac{1}{n}\sum_{i=1}^n L(y_i, F(x_k)) = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 }$$

    The term $y_i - \hat{y}_i = e_{k}$ is known as the residual$\\$

    $$ \mathcal{L} = \frac{1}{n}\sum_{i=1}^n (e_k)^2 = \frac{1}{n} [(e_1)^2 +(e_2)^2
    + (e_n)^2 + ... + (e_n)^2 ] = \frac{1}{n}\begin{bmatrix} e_1 & e_2 & \cdots & e_n \end{bmatrix}
    \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_n \end{bmatrix} = \frac{1}{n}e^Te$$

    $$ \mathcal{L} = \frac{1}{n}e^Te = \frac{1}{n}(Y - \hat{Y})^T(Y - \hat{Y}) = \frac{1}{n}(Y^T - \hat{Y}^T)(Y - \hat{Y}) =
    \frac{1}{n}(Y^TY - Y^T\hat{Y} - \hat{Y}^TY + \hat{Y}^T\hat{Y})$$
</div>
  <h1 class="section__subtitle">The Loss Function of OLS Regression</h1>  
    <div>
    Substituting $\quad \hat{Y} = X\beta \quad$ from equation [2] in above equation $\\$
    $$ \mathcal{L} = \frac{1}{n}(Y^TY - Y^TX\beta - (X\beta)^TY + (X\beta)^T(X\beta)) $$

    Note that $(X\beta)^T = \beta^TX^T$: $\\$
    $$ \mathcal{L} = \frac{1}{n}(Y^TY - Y^TX\beta - \beta^TX^TY + \beta^TX^TX\beta) $$

    Since $Y^TX\beta$ is a scalar, $Y^TX\beta = (Y^TX\beta)^T = \beta^TX^TY$: $\\$
    $$ \mathcal{L} = \frac{1}{n}(Y^TY - 2Y^TX\beta + \beta^TX^TX\beta)$$

    The Loss function $\mathcal{L}$ can be expressed as a function of $Y$, $X$ and $\beta$: $\\$

    $$ \mathcal{L}(Y, X, \beta) = \frac{1}{n}(Y^TY - 2Y^TX\beta + \beta^TX^TX\beta) \tag{3}$$

    where: $\\$
    $Y \in \mathbb{R}^{n \times 1}$ is the vector of observed values $\\$
    $X \in \mathbb{R}^{n \times (m+1)}$ is the design matrix $\\$
    $\beta \in \mathbb{R}^{(m+1) \times 1}$ is the parameter vector $\\$
    </div>

  <h1 class="section__subtitle">Minimization of Loss Function</h1>  
    <div>
    To find the minimum of $\mathcal{L}(Y, X, \beta)$, we will take differentiation of $\mathcal{L}$, since $Y, X$ are constants for given dataset, we will take differentiation with respect to $\beta$. 
    Essentially we are asking which value of $\beta$ will make $\mathcal{L}$ minimum , i.e
    $$\frac{\partial \mathcal{L} }{\partial \beta} = 0$$ 

    $$ \frac{\partial \mathcal{L} }{\partial \beta} = \frac{1}{n}(-2X^TY + 2X^TX\beta) = 0 \tag{4}$$

    Solving for $\beta$: $\\$

    $$ -2X^TY + 2X^TX\beta = 0 $$
    $$ 2X^TX\beta = 2X^TY $$
    $$ X^TX\beta = X^TY $$
    $$ \beta = (X^TX)^{-1}X^TY \tag{5}$$

    <div class="item__title">Wait, </div>
    Since $\mathcal{L}: \mathbb{R}^{(m+1)} \to \mathbb{R}$ maps a vector to a scalar, we can use not simply use partial
    differentiation, we will must use the gradient operator: $\\$

    Starting from equation (2): $\\$
    
    $$ \mathcal{L}(Y, X, \beta) = \frac{1}{n}(Y^TY - 2Y^TX\beta + \beta^TX^TX\beta) $$

    Taking the gradient with respect to $\beta$: $\\$

    $$ \nabla_\beta \mathcal{\mathcal{L} } = \frac{1}{n}\nabla_\beta(Y^TY - 2Y^TX\beta + \beta^TX^TX\beta) $$

    Using matrix calculus rules: $\\$
    $$ \nabla_\beta(Y^TY) = 0 \quad \text{(constant term)} $$
    $$ \nabla_\beta(-2Y^TX\beta) = -2X^TY $$
    $$ \nabla_\beta(\beta^TX^TX\beta) = 2X^TX\beta $$

    Therefore: $\\$
    $$ \nabla_\beta \mathcal{L} = \frac{1}{n}(0 - 2X^TY + 2X^TX\beta) = \frac{1}{n}(-2X^TY + 2X^TX\beta) \tag{6} $$

    Solving for $\beta$: $\\$

    $$ -2X^TY + 2X^TX\beta = 0 $$
    $$ 2X^TX\beta = 2X^TY $$
    $$ X^TX\beta = X^TY $$
    $$ \beta = (X^TX)^{-1}X^TY \tag{7}$$
    </div>
    <h1 class="section__subtitle">How To Verify The Minimum Value Of Loss Function</h1>  
    <div>
    To verify this is a minimum, we need to show that the Hessian matrix is positive definite: $\\$

    The Hessian matrix is the gradient of the gradient: $\\$
    $$ H = \nabla^2_\beta \mathcal{L} = \nabla_\beta(\nabla_\beta L) $$

    From equation (3): $\\$
    $$ \nabla_\beta \mathcal{L} = \frac{1}{n}(-2X^TY + 2X^TX\beta) $$

    Taking gradient again: $\\$
    $$ H = \nabla^2_\beta \mathcal{L} = \frac{2}{n}X^TX $$

    This is a minimum if $H$ is positive definite. $H$ is positive definite if: $\\$
    1. $X^TX$ is symmetric (which it is by construction) $\\$
    2. For any non-zero vector $v$, $v^T(X^TX)v > 0$ $\\$

    When $X$ has full column rank: $\\$
    $$ v^T(X^TX)v = (Xv)^T(Xv) = \|Xv\|^2 > 0 \quad \text{for all } v \neq 0 $$

    Therefore, $\hat{\beta} = (X^TX)^{-1}X^TY$ is indeed a global minimum when $X$ has full column rank. $\\$

  </div>

</section>  </main>
<!--==================== FOOTER ====================-->
<footer class="footer">
    <div class="line-1 ">
        <small>Designed &amp; Built by Rahul Das   </small>
        <span class="pr-area">
            <a href=https://github.com/RahulDas-dev/profile target="_blank">
                <svg class=" stroke-neutral-600 dark:stroke-neutral-100 " viewBox="0 0 24 24" width="14" height="14" stroke-width="0.25" >
                    <path d="M6 5C6 4.44772 6.44772 4 7 4C7.55228 4 8 4.44772 8 5C8 5.55228 7.55228 6 7 6C6.44772 6 6 5.55228 6 5ZM8 7.82929C9.16519 7.41746 10 6.30622 10 5C10 3.34315 8.65685 2 7 2C5.34315 2 4 3.34315 4 5C4 6.30622 4.83481 7.41746 6 7.82929V16.1707C4.83481 16.5825 4 17.6938 4 19C4 20.6569 5.34315 22 7 22C8.65685 22 10 20.6569 10 19C10 17.7334 9.21506 16.6501 8.10508 16.2101C8.45179 14.9365 9.61653 14 11 14H13C16.3137 14 19 11.3137 19 8V7.82929C20.1652 7.41746 21 6.30622 21 5C21 3.34315 19.6569 2 18 2C16.3431 2 15 3.34315 15 5C15 6.30622 15.8348 7.41746 17 7.82929V8C17 10.2091 15.2091 12 13 12H11C9.87439 12 8.83566 12.3719 8 12.9996V7.82929ZM18 6C18.5523 6 19 5.55228 19 5C19 4.44772 18.5523 4 18 4C17.4477 4 17 4.44772 17 5C17 5.55228 17.4477 6 18 6ZM6 19C6 18.4477 6.44772 18 7 18C7.55228 18 8 18.4477 8 19C8 19.5523 7.55228 20 7 20C6.44772 20 6 19.5523 6 19Z" fill="#4b5563"/>
                </svg>
            </a>
        </span>
    </div>
    <div>
        <small>A framework-free, responsive design bundled with vite.</small>
    </div>
</footer></body>
</html>
